{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Imports\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import random\n",
    "\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Lambda\n",
    "from keras.initializers import Identity, RandomNormal\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# TensorFlow initialization\n",
    "import tensorflow as tf\n",
    "session = tf.Session()\n",
    "from keras import backend as K\n",
    "K.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createKochTwin(inputDim, normalStddev, biasMean, lambdaConv, lambdaDense):\n",
    "\t'''\n",
    "\tBuilds a Siamese twin to be shared by the two inputs. Architecture used in Koch et al. (2015).\n",
    "\n",
    "\t@param: inputDim - shape of the input image\n",
    "\t@param: normalStddev - standard deviation of the zero mean Gaussian to be used for the weight initialization\n",
    "\t@param: biasMean - mean of the Gaussian to be used for bias initialization\n",
    "\t@param: lambdaConv - regularization parameter for the convolutional layers\n",
    "\t@param: lambdaDense - regularization parameter for the dense layers\n",
    "\t'''\n",
    "\ttwin = Sequential()\n",
    "\ttwin.add(Conv2D(64, (10, 10), activation='relu', input_shape=inputDim, kernel_initializer=RandomNormal(stddev=normalStddev), kernel_regularizer=l2(lambdaConv)))\n",
    "\ttwin.add(MaxPooling2D()) # default pool size pool_size=(2, 2)\n",
    "\n",
    "\ttwin.add(Conv2D(128, (7, 7), activation='relu', kernel_initializer=RandomNormal(stddev=normalStddev), kernel_regularizer=l2(lambdaConv), bias_initializer=RandomNormal(mean=biasMean, stddev=normalStddev)))\n",
    "\ttwin.add(MaxPooling2D())\n",
    "\n",
    "\ttwin.add(Conv2D(128, (4, 4), activation='relu', kernel_initializer=RandomNormal(stddev=normalStddev), kernel_regularizer=l2(lambdaConv), bias_initializer=RandomNormal(mean=biasMean, stddev=normalStddev)))\n",
    "\ttwin.add(MaxPooling2D())\n",
    "\n",
    "\ttwin.add(Conv2D(256, (4, 4), activation='relu', kernel_initializer=RandomNormal(stddev=normalStddev), kernel_regularizer=l2(lambdaConv), bias_initializer=RandomNormal(mean=biasMean, stddev=normalStddev)))\n",
    "\ttwin.add(Flatten())\n",
    "\ttwin.add(Dense(4096, activation='sigmoid', kernel_regularizer=l2(lambdaDense), kernel_initializer=RandomNormal(stddev=normalStddev), bias_initializer=RandomNormal(mean=biasMean, stddev=normalStddev)))\n",
    "\treturn twin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain results faster and to prevent overfitting, instead of the architecture from Koch et al, the following model is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createCompressedTwin(inputDim, normalStddev, biasMean, lambdaConv, lambdaDense):\n",
    "\t'''\n",
    "\tBuilds a Siamese twin to be shared by the two inputs. A trimmed network architecture based on Koch et al. (2015).\n",
    "\n",
    "\t@param: inputDim - shape of the input image\n",
    "\t@param: normalStddev - standard deviation of the zero mean Gaussian to be used for the weight initialization\n",
    "\t@param: biasMean - mean of the Gaussian to be used for bias initialization\n",
    "\t@param: lambdaConv - regularization parameter for the convolutional layers\n",
    "\t@param: lambdaDense - regularization parameter for the dense layers\n",
    "\t'''\n",
    "\ttwin = Sequential()\n",
    "\ttwin.add(Conv2D(16, (6, 6), activation='relu', input_shape=inputDim, kernel_initializer=RandomNormal(stddev=normalStddev), kernel_regularizer=l2(lambdaConv)))\n",
    "\ttwin.add(MaxPooling2D()) # default pool size pool_size=(2, 2)\n",
    "\n",
    "\ttwin.add(Conv2D(32, (4, 4), activation='relu', kernel_initializer=RandomNormal(stddev=normalStddev), kernel_regularizer=l2(lambdaConv), bias_initializer=RandomNormal(mean=biasMean, stddev=normalStddev)))\n",
    "\ttwin.add(MaxPooling2D())\n",
    "\n",
    "\ttwin.add(Conv2D(32, (4, 4), activation='relu', kernel_initializer=RandomNormal(stddev=normalStddev), kernel_regularizer=l2(lambdaConv), bias_initializer=RandomNormal(mean=biasMean, stddev=normalStddev)))\n",
    "\t\n",
    "\ttwin.add(Conv2D(64, (2, 2), activation='relu', kernel_initializer=RandomNormal(stddev=normalStddev), kernel_regularizer=l2(lambdaConv), bias_initializer=RandomNormal(mean=biasMean, stddev=normalStddev)))\n",
    "\ttwin.add(Flatten())\n",
    "\ttwin.add(Dense(256, activation='sigmoid', kernel_regularizer=l2(lambdaDense), kernel_initializer=RandomNormal(stddev=normalStddev), bias_initializer=RandomNormal(mean=biasMean, stddev=normalStddev)))\n",
    "\treturn twin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l1Distance(vects):\n",
    "\t'''\n",
    "\tFinds the component wise l1 distance to join the twins. To be used in the Lambda layer definition.\n",
    "\n",
    "\t@param: vects - pair of images processed through the Siamese network\n",
    "\n",
    "\t@return: absolute value of the difference between the two element in vects\n",
    "\t'''\n",
    "\treturn K.abs(vects[0] - vects[1])\n",
    "\n",
    "def l1OutputShape(shapes):\n",
    "\t'''\n",
    "\tReturns the shape of the resulting after the twins are joined. To be used in the Lambda layer definition.\n",
    "\n",
    "\t@param: shapes - shapes of the pair of images processed through the Siamese network\n",
    "\n",
    "\t@return: shapes of the first processed image\n",
    "\t'''\n",
    "\treturn shapes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the 40 classes in the dataset, 38 are used for training and 1 each for validation and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "loadedData = fetch_olivetti_faces()\n",
    "numSamplesPerClass = 10\n",
    "numTrainClasses = 38\n",
    "uniqueClasses = list(set(loadedData.target))\n",
    "\n",
    "IMG_SIZE = 64 # Using the actual size\n",
    "inputDim = (IMG_SIZE, IMG_SIZE, 1)\n",
    "allImages = loadedData.images \n",
    "# Reshaping the data for ease of indexing\n",
    "allImagesByClass = np.reshape(allImages, (len(uniqueClasses), numSamplesPerClass, IMG_SIZE, IMG_SIZE, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is augmented by making stochastic affine transformations. These include shifting, both horizontally as well as vertically, horizontal flipping, and rotating the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_gen_args = dict(\n",
    "\trotation_range=10,\n",
    "\twidth_shift_range=0.05,\n",
    "\theight_shift_range=0.05,\n",
    "\thorizontal_flip=True)\n",
    "\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "maxDraws = 6 # Number of transformations per image\n",
    "augmentedImagesByClass = [[] for k in range(40)]\n",
    "for nClass in uniqueClasses:\n",
    "\tnumDraws = 0\n",
    "\temptArr = []\n",
    "\tfor _x, _y in image_datagen.flow(allImagesByClass[nClass], [nClass]*numSamplesPerClass):\n",
    "\t\temptArr += _x\n",
    "\t\tnumDraws += 1\n",
    "\t\tif numDraws == 1:\n",
    "\t\t\taugmentedImagesByClass[nClass] = _x\n",
    "\t\telif numDraws <= maxDraws:\n",
    "\t\t\taugmentedImagesByClass[nClass] = np.concatenate((augmentedImagesByClass[nClass], _x))\n",
    "\t\tif numDraws >= maxDraws:\n",
    "\t\t\tbreak\n",
    "augmentedImagesByClass = np.array(augmentedImagesByClass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the limited number of classes in the training data, a leave one out cross validation is performed. This way each class appears in the test set once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numFolds = len(uniqueClasses)\n",
    "# Lists to store the results of the cross validation folds\n",
    "valScoresList = [[] for k in range(numFolds)]\n",
    "test1ScoresList = [[] for k in range(numFolds)]\n",
    "test2ScoresList = [[] for k in range(numFolds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparmaters used for the network\n",
    "lambdaConv = 1e-4\n",
    "lambdaDense = 1e-4\n",
    "normalStddev = 1e-2\n",
    "biasMean = 0.5\n",
    "numEpochs = 10\n",
    "miniBatchSize = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross validation loop\n",
    "for testClass in range(numFolds):\n",
    "\ttrainClasses = uniqueClasses[:testClass]+uniqueClasses[testClass+1:]\n",
    "\trandClass = random.randrange(1, numTrainClasses) # Pick a random class for validation set\n",
    "\tvalClass = trainClasses.pop(randClass)\n",
    "\n",
    "\t# Creating training set\n",
    "\t# Training data has equal distribution of same and different class images\n",
    "\ttrainPairs = []\n",
    "\ttrainLabels = []\n",
    "\tfor nClass in trainClasses:\n",
    "\t\tfor nSample1, nSample2 in combinations(range(numSamplesPerClass*6),2):\n",
    "\t\t\ttrainPairs += [[augmentedImagesByClass[nClass, nSample1], augmentedImagesByClass[nClass, nSample2]]]\n",
    "\t\t\tinc = random.randrange(1, numTrainClasses)\n",
    "\t\t\tidx = (nClass + inc) % numTrainClasses\n",
    "\t\t\tnClass2 = trainClasses[idx]\n",
    "\t\t\ttrainPairs += [[augmentedImagesByClass[nClass, nSample1], augmentedImagesByClass[nClass2, nSample2]]]\n",
    "\t\t\ttrainLabels += [1, 0]\n",
    "\ttrainPairs = np.array(trainPairs)\n",
    "\ttrainLabels = np.array(trainLabels)\n",
    "\n",
    "\t# Creating validation set\n",
    "\t# Validation data has equal distribution of same and different class images\n",
    "\tvalPairs = []\n",
    "\tvalLabels = []\n",
    "\tfor nSample1, nSample2 in combinations(range(numSamplesPerClass),2):\n",
    "\t\tvalPairs += [[allImagesByClass[valClass, nSample1], allImagesByClass[valClass, nSample2]]]\n",
    "\t\tinc = random.randrange(1, numTrainClasses)\n",
    "\t\tnClass2 = trainClasses[inc % numTrainClasses]\n",
    "\t\tvalPairs += [[allImagesByClass[valClass, nSample1], allImagesByClass[nClass2, nSample2]]]\n",
    "\t\tvalLabels += [1, 0]\n",
    "\tvalPairs = np.array(valPairs)\n",
    "\tvalLabels = np.array(valLabels)\n",
    "\n",
    "\t# Creating test set 1\n",
    "\t# Constructed by drawing a pair of images from held out test class\n",
    "\ttestPairs = []\n",
    "\ttestLabels = []\n",
    "\tfor nSample1, nSample2 in combinations(range(numSamplesPerClass),2):\n",
    "\t\ttestPairs += [[allImagesByClass[testClass, nSample1], allImagesByClass[testClass, nSample2]]]\n",
    "\t\ttestLabels += [1]\n",
    "\ttestPairs = np.array(testPairs)\n",
    "\ttestLabels = np.array(testLabels)\n",
    "\n",
    "\t# Creating test set 2\n",
    "\t# Constructed by drawing one image from the held out test class and another from any other class\n",
    "\ttestPairs2 = []\n",
    "\ttestLabels2 = []\n",
    "\tfor nClass in trainClasses:\n",
    "\t\tfor nSample1 in range(numSamplesPerClass):\n",
    "\t\t\tfor nSample2 in range(numSamplesPerClass):\n",
    "\t\t\t\ttestPairs2 += [[allImagesByClass[testClass, nSample1], allImagesByClass[nClass, nSample2]]]\n",
    "\t\t\t\ttestLabels2 += [0]\n",
    "\ttestPairs2 = np.array(testPairs2)\n",
    "\ttestLabels2 = np.array(testLabels2)\n",
    "\n",
    "\t# Network model construction\n",
    "\tbaseTwin = createCompressedTwin(inputDim, normalStddev, biasMean, lambdaConv, lambdaDense)\n",
    "\tinputL = Input(inputDim)\n",
    "\tinputR = Input(inputDim)\n",
    "\tprocessedL = baseTwin(inputL)\n",
    "\tprocessedR = baseTwin(inputR)\n",
    "\n",
    "\tfeatureVector = Lambda(l1Distance, output_shape=l1OutputShape)([processedL, processedR])\n",
    "\tsigmoidOutput = Dense(1, activation='sigmoid', bias_initializer=RandomNormal(mean=biasMean, stddev=normalStddev))(featureVector)\n",
    "\tsiameseModel = Model(input=[inputL, inputR], output=sigmoidOutput)\n",
    "\tsiameseModel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\t# Training loop\n",
    "\tfor k in range(numEpochs):\n",
    "\t\tprint 'Epoch', k\n",
    "\t\tsiameseModel.fit([trainPairs[:, 0], trainPairs[:, 1]], trainLabels, batch_size=128, epochs=1, verbose=0)\n",
    "\t\t\n",
    "\t\tvalScores = siameseModel.evaluate([valPairs[:, 0], valPairs[:, 1]], valLabels, verbose=0)\n",
    "\t\tvalScoresList[testClass].append(valScores[1])\n",
    "\n",
    "\t\ttest1Scores = siameseModel.evaluate([testPairs[:, 0], testPairs[:, 1]], testLabels, verbose=0)\n",
    "\t\ttest1ScoresList[testClass].append(test1Scores[1])\n",
    "\n",
    "\t\ttest2Scores = siameseModel.evaluate([testPairs2[:, 0], testPairs2[:, 1]], testLabels2, verbose=0)\n",
    "\t\ttest2ScoresList[testClass].append(test2Scores[1])\n",
    "\n",
    "\t# Caching prediction at the end of training to get RoC/DET curve \n",
    "\tpositiveSamplesScores = siameseModel.predict([testPairs[:, 0], testPairs[:, 1]])\n",
    "\tnegativeSamplesScores = siameseModel.predict([testPairsInTrain[:, 0], testPairsInTrain[:, 1]])\n",
    "\n",
    "\tsiamesePrediction = np.concatenate((positiveSamplesScores,negativeSamplesScores))\n",
    "\ttrueLabel = np.concatenate((testLabels, testLabelsInTrain))\n",
    "\tfpr, tpr, thresholds = roc_curve(trueLabel, siamesePrediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is done for only 10 epochs due to the limited computational resources. Similar to Koch et al., the parameters of the model at the best epoch according to the one-shot validation error is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accurayByClass = [None for k in range(numFolds)]\n",
    "for testClass in range(numFolds):\n",
    "\tepochStop = valScoresList.index(max(valScoresList[testClass]))\n",
    "\taccurayByClass[testClass] = (test1ScoresList[testClass][epochStop]+test2ScoresList[testClass][epochStop])/2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trained on the original dataset, without any augmentation, this model obtained an accuracy of 68% averaged over the 40 cross-validation folds. After dataset augmentation, this model obtains a cross-validated accuracy of 73%.\n",
    "\n",
    "While this accuracy is about 20 percentage points below the accuracy reported by Koch et al., this could be due to the amount of training data being limited, not using the Bayesian hyperparameter optimization, and not training for more epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "From the prediction made by the model, it was observed that the model was able to detect that two images were from different classes with an average accuracy of 95%. However, the model was performing poorly in detecting that two images were from the same class. This imbalance is caused by the threshold used for classifcation. Instead of using a threshold value of 0.5, were the assumption is that the distribution of the two classes is symmetric, it could be optimized by analyzing the detection error tradeoff (DET) curve.\n",
    "\n",
    "The figure below shows DET curves for three different cross-validation folds. From this it can be seen that changing the threshold (by using a validation set) can potentially reduce the error by 15 to 25 percentage point. These curves show promise that choosing an optimal threshold can significantly improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 20.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEWCAYAAACjYXoKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZoU5mZiZWWFOJmZmVpiTiZmZFfb/AVQoj9uJnIHlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x8e4cd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lineNumber = 0\n",
    "for line in open('rocResults.txt'):\n",
    "\tcontent = line.strip().split(',')\n",
    "\tif lineNumber%4==0:\n",
    "\t\tfpr = [float(elem)*100 for elem in content]\n",
    "\telif lineNumber%4==1:\n",
    "\t\tmdr = [100.0-float(elem)*100 for elem in content]\n",
    "\telif lineNumber%4==2:\n",
    "\t\tthreshold = [float(elem) for elem in content]\n",
    "\telif lineNumber%4==3:\n",
    "\t\tplt.plot(fpr, mdr)\n",
    "\tlineNumber = lineNumber + 1\n",
    "plt.ylabel('Missed Detection Rate (%)')\n",
    "plt.xlabel('False Positive Rate (%)')\n",
    "plt.title('Detection Error Tradeoff Curve')\n",
    "plt.ylim([0.0,20.0])\n",
    "plt.xlim([0.0,20.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways to improve performance:\n",
    "1. Training for more epochs\n",
    "2. Making a larger augmented dataset\n",
    "3. Choosing classification threshold based on the DET curve\n"
    "4. Optimizing hyperparameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
